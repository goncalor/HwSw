Através de duas técnicas de aceleração --- paralelização do processamento de dados e a criação de \textit{hardware} dedicado para as contagens de caracteres --- conseguiu-se acelerar a compressão de texto com codificação de Huffman. Embora se tenha considerado algumas simplificações para demonstrar o funcionamento, consideramos que são de pouca importância e que seriam facilmente resolvidas numa aplicação real.

Quanto à escolha de secções para acelerar cobrimos as mais morosas, o que não implica que o sistema e o código não possam ser mais optimizados. Uma pequena optimização seria, por exemplo, em vez de os processadores 1 e 3 escreverem na memória local e depois copiarem os dados para a memória interna, escreverem directamente na memória interna.

Os resultados conseguidos estão dentro do esperado sendo que se conseguiu um \textit{speedup} superior a 3 e inferior a 4. O \textit{speedup} depende do ficheiro a comprimir e foi observado que alguns \textit{cores} podem demorar mais que outros porque podem ter mais ``azar'' e ficar com uma parte do ficheiro mais lenta de comprimir. Consequentemente, um dos processadores pode prejudicar aquele que seria o tempo de execução global se todos os processadores tivessem partes do ficheiro igualmente difíceis de comprimir.

Com este trabalho aprendeu-se a projectar um sistema multi-processador e a resolver alguns dos problemas inerentes a um sistema deste tipo, como sejam os mecanismos de sincronização e partilha de dados entre os vários processadores. Aprendeu-se também a melhor utilizar as ferramentas de projecto.